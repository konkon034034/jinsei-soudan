#!/usr/bin/env python3
from dotenv import load_dotenv
load_dotenv()

import os
import json
import gspread
import requests
import tempfile
from datetime import datetime
from google.oauth2.service_account import Credentials
from google.oauth2.credentials import Credentials as OAuthCredentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload

SCOPES = [
    'https://www.googleapis.com/auth/spreadsheets',
    'https://www.googleapis.com/auth/youtube.upload'
]
SPREADSHEET_ID = '15_ixYlyRp9sOlS0tdklhz6wQmwRxWlOL9cPndFWwOFo'

# Total number of channels
TOTAL_CHANNELS = 27

# Cache for channel info from spreadsheet
_channel_info_cache = None

def get_channel_info_from_sheet(sh):
    """Read channel information from '27ãƒãƒ£ãƒ³ãƒãƒ«ä¸€è¦§' sheet."""
    global _channel_info_cache

    if _channel_info_cache is not None:
        return _channel_info_cache

    try:
        ws = sh.worksheet('27ãƒãƒ£ãƒ³ãƒãƒ«ä¸€è¦§')
        all_data = ws.get_all_values()

        channel_info = {}
        for row in all_data[1:]:  # Skip header
            if len(row) >= 3:
                try:
                    token_num = int(row[0])
                    email = row[1]
                    channel_name = row[2]
                    channel_info[token_num] = {
                        'email': email,
                        'name': channel_name if channel_name != 'ï¼ˆæœªè¨­å®šï¼‰' else None
                    }
                except (ValueError, IndexError):
                    continue

        _channel_info_cache = channel_info
        return channel_info
    except Exception as e:
        print(f"  âš ï¸ ãƒãƒ£ãƒ³ãƒãƒ«æƒ…å ±èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def get_channel_name(sh, channel_id):
    """Get channel name for the given channel ID."""
    channel_info = get_channel_info_from_sheet(sh)

    # Get channel number from environment or use channel_id
    channel_number = int(os.environ.get('CHANNEL_NUMBER', channel_id))

    info = channel_info.get(channel_number, {})
    return info.get('name')

# Fallback channel names (used when spreadsheet doesn't have the info)
CHANNELS = {
    1: "æ˜­å’Œã®å®ç®±", 2: "æ‡ã‹ã—ã®æ­Œè¬¡æ›²ch", 3: "æ€ã„å‡ºãƒ©ãƒ³ã‚­ãƒ³ã‚°", 4: "æ˜­å’Œã‚¹ã‚¿ãƒ¼åé‘‘",
    5: "æ¼”æ­Œã®æ®¿å ‚", 6: "éŠ€å¹•ã®æ€ã„å‡º", 7: "æ‡ãƒ¡ãƒ­å¤©å›½", 8: "æœãƒ‰ãƒ©å¤§å…¨é›†",
    9: "æ˜­å’Œãƒ—ãƒ¬ã‚¤ãƒãƒƒã‚¯", 10: "æ˜­å’Œãƒã‚¹ã‚¿ãƒ«ã‚¸ã‚¢", 11: "é»„é‡‘æ™‚ä»£ch", 12: "æ˜­å’Œãƒ‰ãƒ©ãƒåŠ‡å ´",
    13: "æˆ¦å¾Œæ—¥æœ¬ã®è¨˜æ†¶", 14: "æ˜­å’Œã®å­¦æ ¡", 15: "åˆ¶æœã¨æ ¡å‰‡ch", 16: "æ˜­å’Œã®é£Ÿå“",
    17: "æ˜­å’Œã‚°ãƒ«ãƒ¡å›³é‘‘", 18: "æ˜­å’ŒCMåšè¦§ä¼š", 19: "CMã‚½ãƒ³ã‚°å¤§å…¨", 20: "æ˜­å’Œã®æš®ã‚‰ã—",
    21: "æ˜­å’Œã®å®¶æ—", 22: "ãŠã—ã‚ƒã‚Œè¡—é“", 23: "æ˜­å’Œãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³", 24: "ãƒ¬ãƒˆãƒ­ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼",
    25: "æ˜­å’Œã‚¹ãƒãƒ¼ãƒ„ä¼èª¬", 26: "æ˜­å’Œãƒãƒ©ã‚¨ãƒ†ã‚£", 27: "æ¿€å‹•ã®æ˜­å’Œå²"
}

def get_credentials():
    creds_json = os.environ.get('GOOGLE_CREDENTIALS_JSON')
    creds_dict = json.loads(creds_json)
    return Credentials.from_service_account_info(creds_dict, scopes=SCOPES)

def get_pending_neta(sh, channel_id=None):
    """Get pending neta from spreadsheet.

    Args:
        sh: Spreadsheet object
        channel_id: Optional channel ID to filter by. If None, returns first pending neta.

    Returns:
        dict with neta info or None if no pending neta found
    """
    ws = sh.worksheet('ãƒã‚¿ç®¡ç†')
    all_data = ws.get_all_values()
    for i, row in enumerate(all_data[1:], start=2):
        if len(row) >= 6 and row[5] == 'æœªä½œæˆ':
            row_channel_id = int(row[1]) if row[1].isdigit() else 0
            # If channel_id specified, only return neta for that channel
            if channel_id is not None and row_channel_id != channel_id:
                continue
            return {
                'row_num': i,
                'neta_id': row[0],
                'channel_id': row_channel_id,
                'category': row[2],
                'title': row[3],
                'ranking_num': int(row[4]) if row[4].isdigit() else 15
            }
    return None

def generate_ranking_content(neta):
    api_key = os.environ.get('CLAUDE_API_KEY')
    
    prompt = f"""ã‚ãªãŸã¯æ˜­å’Œæ™‚ä»£ã‚’æ‡ã‹ã—ã‚€èªã‚Šéƒ¨ã§ã™ã€‚
ä»¥ä¸‹ã®å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã€ãƒ©ãƒ³ã‚­ãƒ³ã‚°å‹•ç”»ã®ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åŸç¨¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {neta['title']}
ãƒ©ãƒ³ã‚­ãƒ³ã‚°æ•°: TOP{neta['ranking_num']}

ã€é‡è¦ãªé›°å›²æ°—â‘ ï¼šæ™‚ã®æµã‚Œã®åˆ‡ãªã•ã€‘
å…¨ä½“ã‚’é€šã—ã¦ã€Œæ™‚ã®æµã‚Œã®åˆ‡ãªã•ã€ã‚’æ„Ÿã˜ã•ã›ã¦ãã ã•ã„ã€‚
- ã€Œã‚ã®é ƒã®è¼ãã¯ã€ä»Šã‚‚ç§ãŸã¡ã®å¿ƒã®ä¸­ã«ç”Ÿãã¦ã„ã¾ã™ã€
- ã€Œã™ã£ã‹ã‚Šæ™‚é–“ãŒçµŒã£ã¦ã—ã¾ã„ã¾ã—ãŸã­ã€
- ã€Œæ™‚ã®æµã‚Œã¯åˆ‡ãªã„ã‚‚ã®ã§ã™ãŒã€ã ã‹ã‚‰ã“ãæ€ã„å‡ºã¯ç¾ã—ã„ã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€
- ã€Œã‚ã‚Œã‹ã‚‰ä½•åå¹´...è¡—ã®æ™¯è‰²ã¯å¤‰ã‚ã£ã¦ã‚‚ã€ã‚ã®é ƒã®è¨˜æ†¶ã¯è‰²è¤ªã›ã¾ã›ã‚“ã€
- ã€Œä»Šã¯ã‚‚ã†è¦‹ã‚‹ã“ã¨ãŒã§ããªã„é¢¨æ™¯ã§ã™ãŒ...ã€

ã€é‡è¦ãªé›°å›²æ°—â‘¡ï¼šè¦–è´è€…ã‚’ç§°ãˆã€è‡ªåˆ†ã”ã¨ã«ã€‘
è¦‹ã¦ã„ã‚‹è¦–è´è€…è‡ªèº«ã‚’ç§°ãˆã€ã€Œè‡ªåˆ†ã®äººç”Ÿã ã€ã¨æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
- ã€Œã“ã‚Œã‚’ã”è¦§ã®ã‚ãªãŸã‚‚ã€ãã£ã¨ã‚ã®æ™‚ä»£ã‚’æ‡¸å‘½ã«ç”Ÿãã¦ã“ã‚‰ã‚ŒãŸã®ã§ã™ã­ã€
- ã€Œã‚ãªãŸãŒã„ãŸã‹ã‚‰ã“ãã€ã‚ã®æ™‚ä»£ã¯è¼ã„ã¦ã„ãŸã®ã§ã™ã€
- ã€Œä»Šæ—¥ã¾ã§æ­©ã‚“ã§ã“ã‚‰ã‚ŒãŸäººç”Ÿã€æœ¬å½“ã«ç´ æ™´ã‚‰ã—ã„ã‚‚ã®ã§ã™ã€
- ã€Œã‚ãªãŸã®è¨˜æ†¶ã®ä¸­ã«ã‚‚ã€ãã£ã¨ã“ã‚“ãªæ€ã„å‡ºãŒã‚ã‚‹ã®ã§ã¯ãªã„ã§ã—ã‚‡ã†ã‹ã€
- ã€Œã“ã®æ™‚ä»£ã‚’çŸ¥ã‚‹ã‚ãªãŸã ã‹ã‚‰ã“ãã€åˆ†ã‹ã‚‹å–œã³ãŒã‚ã‚Šã¾ã™ã‚ˆã­ã€
- ã€Œã‚ãªãŸãŒç©ã¿é‡ã­ã¦ããŸæ—¥ã€…ãŒã€ã©ã‚Œã»ã©å°Šã„ã‚‚ã®ã‹ã€
- ã€Œå…±ã«æ™‚ä»£ã‚’ç”ŸããŸç§ãŸã¡ã ã‹ã‚‰ã“ãã€åˆ†ã‹ã¡åˆãˆã‚‹æ€ã„å‡ºã§ã™ã€

è¦–è´è€…ã«ã€Œç§ã®äººç”Ÿã‚’èªã‚ã¦ã‚‚ã‚‰ãˆãŸã€ã€Œç§ã®æ™‚ä»£ã¯ä¾¡å€¤ãŒã‚ã£ãŸã€ã¨æ„Ÿã˜ã•ã›ã¦ãã ã•ã„ã€‚

æ¡ä»¶:
- 60æ­³ä»¥ä¸Šã®å¥³æ€§è¦–è´è€…å‘ã‘
- å„é †ä½ã«ã¤ã„ã¦2-3æ–‡ã§è§£èª¬
- ã—ã¿ã˜ã¿ã¨ã—ãŸã€åˆ‡ãªãã‚‚æ¸©ã‹ã„èªã‚Šå£
- è¦–è´è€…ã«ç›´æ¥èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ï¼ˆã€Œã‚ãªãŸã€ã€Œçš†ã•ã¾ã€ã‚’ä½¿ã†ï¼‰
- ã€Œã‚ã®é ƒã€ã¨ã€Œä»Šã€ã‚’å¯¾æ¯”ã•ã›ã€æ™‚ä»£ã®ç§»ã‚Šå¤‰ã‚ã‚Šã‚’æ„Ÿã˜ã•ã›ã‚‹
- äºŒåº¦ã¨æˆ»ã‚Œãªã„éå»ã¸ã®æ„›ãŠã—ã•ã‚’è¡¨ç¾
- ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ã¨ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¯ç‰¹ã«æ„Ÿå‚·çš„ã«ã€è¦–è´è€…ã¸ã®æ„Ÿè¬ã‚’è¾¼ã‚ã¦
- 8åˆ†ç¨‹åº¦ã®å‹•ç”»ã«ãªã‚‹åˆ†é‡

ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›:
[ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°]
ï¼ˆè¦–è´è€…ã«èªã‚Šã‹ã‘ã€æ™‚ã®æµã‚Œã‚’æ„Ÿã˜ã•ã›ã‚‹å°å…¥ã€‚30ç§’ç¨‹åº¦ï¼‰

[ç¬¬{neta['ranking_num']}ä½]
é …ç›®å: â—‹â—‹â—‹
è§£èª¬: ï¼ˆ2-3æ–‡ã€‚ã€Œã‚ãªãŸã‚‚è¦šãˆã¦ã„ã¾ã™ã‹ï¼Ÿã€ãªã©è¦–è´è€…ã«å•ã„ã‹ã‘ãªãŒã‚‰ï¼‰

[ç¬¬{neta['ranking_num']-1}ä½]
...

[ç¬¬1ä½]
é …ç›®å: â—‹â—‹â—‹
è§£èª¬: ï¼ˆ2-3æ–‡ã€‚æœ€ã‚‚å°è±¡çš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ï¼‰

[ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°]
ï¼ˆè¦–è´è€…ã¸ã®æ„Ÿè¬ã¨åŠ´ã„ã€æ™‚ã®æµã‚Œã‚’æŒ¯ã‚Šè¿”ã‚Šã€å¿ƒã«æ®‹ã‚‹ã¾ã¨ã‚ã€‚ã€Œã‚ãªãŸã®äººç”Ÿã¯ç´ æ™´ã‚‰ã—ã„ã€ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¾¼ã‚ã¦ï¼‰"""

    url = "https://api.anthropic.com/v1/messages"
    headers = {
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }
    payload = {
        "model": "claude-3-haiku-20240307",
        "max_tokens": 4000,
        "messages": [{"role": "user", "content": prompt}]
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        result = response.json()
        if 'content' in result:
            return result['content'][0]['text']
    except Exception as e:
        print(f"  âš ï¸ åŸç¨¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    return None

def generate_audio_google_tts(text, output_path):
    from gtts import gTTS
    
    try:
        max_chars = 5000
        if len(text) > max_chars:
            text = text[:max_chars]
        
        tts = gTTS(text=text, lang='ja', slow=False)
        tts.save(output_path)
        return True
    except Exception as e:
        print(f"  âš ï¸ éŸ³å£°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
    return False

# Japanese to English keyword mapping for Unsplash search
KEYWORD_MAP = {
    # Categories
    'æ­Œè¬¡æ›²': 'japanese music vintage',
    'æ¼”æ­Œ': 'japanese traditional music',
    'æ˜ ç”»': 'vintage cinema japan',
    'éŠ€å¹•': 'classic movie theater',
    'ãƒ‰ãƒ©ãƒ': 'vintage television japan',
    'æœãƒ‰ãƒ©': 'japanese morning drama vintage',
    'CM': 'vintage advertisement japan',
    'åºƒå‘Š': 'retro advertising',
    'ãƒ•ã‚¡ãƒƒã‚·ãƒ§ãƒ³': 'vintage fashion 1960s 1970s',
    'ãŠã—ã‚ƒã‚Œ': 'retro style fashion',
    'åŒ–ç²§å“': 'vintage cosmetics beauty',
    'ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼': 'retro beauty makeup',
    'é£Ÿå“': 'japanese home cooking vintage',
    'ã‚°ãƒ«ãƒ¡': 'vintage japanese food',
    'å­¦æ ¡': 'vintage school classroom japan',
    'åˆ¶æœ': 'japanese school uniform vintage',
    'ã‚¹ãƒãƒ¼ãƒ„': 'vintage sports japan',
    'ãƒãƒ©ã‚¨ãƒ†ã‚£': 'japanese entertainment vintage',
    'æš®ã‚‰ã—': 'vintage japanese lifestyle',
    'å®¶æ—': 'japanese family vintage',
    'æ˜­å’Œ': 'japan 1960s 1970s vintage',
    'ãƒ¬ãƒˆãƒ­': 'retro vintage nostalgic',
    'æ‡ã‹ã—ã„': 'nostalgic vintage memories',
    'æ€ã„å‡º': 'memories nostalgia vintage',
    'ã‚¹ã‚¿ãƒ¼': 'vintage celebrity star',
    'åé‘‘': 'vintage portrait classic',
    'æˆ¦å¾Œ': 'postwar japan vintage',
    'é»„é‡‘æ™‚ä»£': 'golden age vintage',
    'æ­´å²': 'japanese history vintage',
}

# Fallback search queries for different themes
FALLBACK_QUERIES = [
    'vintage japan street',
    'retro japanese aesthetic',
    'nostalgic sunset',
    'vintage paper texture',
    'retro gradient background',
    'old photograph sepia',
    'cherry blossom vintage',
    'japanese garden peaceful',
]

def translate_to_english_keywords(japanese_text):
    """Convert Japanese title/category to English keywords for Unsplash."""
    keywords = []

    # Check for matching keywords in the text
    for jp_word, en_keywords in KEYWORD_MAP.items():
        if jp_word in japanese_text:
            keywords.append(en_keywords)

    # If no keywords found, use general nostalgic terms
    if not keywords:
        keywords = ['vintage japan nostalgic', 'retro aesthetic']

    # Combine and deduplicate
    combined = ' '.join(keywords[:3])  # Limit to avoid overly complex queries
    return combined

def get_unsplash_images(query, count=10, category=''):
    """Fetch images from Unsplash with English keyword translation."""
    api_key = os.environ.get('UNSPLASH_ACCESS_KEY')
    url = "https://api.unsplash.com/search/photos"
    headers = {"Authorization": f"Client-ID {api_key}"}

    # Translate Japanese to English keywords
    english_query = translate_to_english_keywords(query + ' ' + category)
    print(f"    ğŸ” æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {english_query}")

    # Try multiple search strategies
    search_queries = [
        english_query,
        'vintage japan nostalgic',
        'retro aesthetic background',
    ]

    all_urls = []
    for search_query in search_queries:
        if len(all_urls) >= count:
            break

        params = {
            "query": search_query,
            "per_page": min(count - len(all_urls) + 5, 30),  # Get extra in case of duplicates
            "orientation": "landscape"
        }

        try:
            response = requests.get(url, params=params, headers=headers)
            result = response.json()
            if 'results' in result:
                for img in result['results']:
                    img_url = img['urls']['regular']
                    if img_url not in all_urls:
                        all_urls.append(img_url)
                        if len(all_urls) >= count:
                            break
        except Exception as e:
            print(f"    âš ï¸ æ¤œç´¢ã‚¨ãƒ©ãƒ¼ ({search_query}): {e}")

    return all_urls[:count]

def generate_gradient_background(output_path, width=1280, height=720, style='showa'):
    """Generate a nostalgic gradient background image."""
    from PIL import Image, ImageDraw, ImageFilter
    import random

    # Color palettes for different styles
    palettes = {
        'showa': [
            [(139, 90, 43), (205, 133, 63)],      # Sepia brown
            [(70, 50, 30), (150, 100, 50)],       # Dark brown to tan
            [(80, 60, 40), (180, 140, 90)],       # Earthy tones
            [(100, 70, 50), (200, 160, 100)],     # Warm vintage
        ],
        'sunset': [
            [(255, 94, 77), (255, 154, 139)],     # Coral sunset
            [(255, 123, 84), (255, 184, 140)],    # Orange sunset
            [(180, 80, 100), (255, 150, 120)],    # Pink sunset
        ],
        'nostalgic': [
            [(60, 60, 80), (120, 100, 140)],      # Muted purple
            [(50, 70, 90), (100, 130, 150)],      # Dusty blue
            [(80, 70, 60), (160, 140, 120)],      # Faded vintage
        ]
    }

    # Select palette
    palette_list = palettes.get(style, palettes['showa'])
    colors = random.choice(palette_list)

    # Create gradient image
    img = Image.new('RGB', (width, height))
    draw = ImageDraw.Draw(img)

    # Vertical gradient
    for y in range(height):
        ratio = y / height
        r = int(colors[0][0] + (colors[1][0] - colors[0][0]) * ratio)
        g = int(colors[0][1] + (colors[1][1] - colors[0][1]) * ratio)
        b = int(colors[0][2] + (colors[1][2] - colors[0][2]) * ratio)
        draw.line([(0, y), (width, y)], fill=(r, g, b))

    # Add subtle noise/grain for vintage effect
    noise_img = Image.new('RGB', (width, height))
    noise_draw = ImageDraw.Draw(noise_img)
    for _ in range(5000):
        x = random.randint(0, width - 1)
        y = random.randint(0, height - 1)
        gray = random.randint(0, 30)
        noise_draw.point((x, y), fill=(gray, gray, gray))

    # Blend noise with gradient
    img = Image.blend(img, noise_img, 0.05)

    # Add vignette effect
    vignette = Image.new('L', (width, height), 255)
    vignette_draw = ImageDraw.Draw(vignette)
    for i in range(min(width, height) // 2):
        alpha = int(255 * (1 - (i / (min(width, height) / 2)) ** 2))
        vignette_draw.ellipse(
            [i, i, width - i, height - i],
            outline=alpha
        )
    vignette = vignette.filter(ImageFilter.GaussianBlur(50))

    # Apply vignette
    img_array = list(img.getdata())
    vignette_array = list(vignette.getdata())
    result_data = []
    for i, (pixel, v) in enumerate(zip(img_array, vignette_array)):
        factor = v / 255
        result_data.append((
            int(pixel[0] * factor),
            int(pixel[1] * factor),
            int(pixel[2] * factor)
        ))
    img.putdata(result_data)

    img.save(output_path, 'JPEG', quality=90)
    return True

def get_images_with_fallback(title, category, count, tmpdir):
    """Get images from Unsplash with fallback to generated backgrounds."""
    images = []

    # Try to get images from Unsplash
    search_query = f"{title} {category}"
    image_urls = get_unsplash_images(search_query, count, category)

    # Download Unsplash images
    for i, url in enumerate(image_urls):
        img_path = os.path.join(tmpdir, f"img_{i}.jpg")
        if download_image(url, img_path):
            images.append(img_path)

    # If we don't have enough images, generate fallback backgrounds
    if len(images) < count:
        print(f"    ğŸ¨ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯èƒŒæ™¯ã‚’ç”Ÿæˆä¸­...")
        styles = ['showa', 'sunset', 'nostalgic']
        for i in range(len(images), count):
            img_path = os.path.join(tmpdir, f"fallback_{i}.jpg")
            style = styles[i % len(styles)]
            if generate_gradient_background(img_path, style=style):
                images.append(img_path)

    return images

def download_image(url, output_path):
    try:
        response = requests.get(url, timeout=30)
        if response.status_code == 200:
            with open(output_path, 'wb') as f:
                f.write(response.content)
            return True
    except:
        pass
    return False

def split_script_into_subtitles(script, chars_per_segment=30):
    """Split script into subtitle segments."""
    import re

    # Remove section headers like [ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°], [ç¬¬10ä½] etc.
    script = re.sub(r'\[.*?\]', '', script)
    # Remove empty lines and trim
    lines = [line.strip() for line in script.split('\n') if line.strip()]
    full_text = ' '.join(lines)

    # Split by sentence endings
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ])', full_text)

    subtitles = []
    current = ""

    for i in range(0, len(sentences) - 1, 2):
        sentence = sentences[i] + (sentences[i + 1] if i + 1 < len(sentences) else "")
        sentence = sentence.strip()
        if not sentence:
            continue

        # If sentence is too long, split by chars_per_segment
        if len(sentence) > chars_per_segment * 2:
            words = list(sentence)
            for j in range(0, len(words), chars_per_segment):
                segment = ''.join(words[j:j + chars_per_segment])
                if segment:
                    subtitles.append(segment)
        else:
            if len(current) + len(sentence) > chars_per_segment:
                if current:
                    subtitles.append(current)
                current = sentence
            else:
                current = (current + " " + sentence).strip() if current else sentence

    if current:
        subtitles.append(current)

    return subtitles


def create_video_with_moviepy(audio_path, images, title, output_path, script=None):
    from moviepy import (
        AudioFileClip, ImageClip,
        concatenate_videoclips, ColorClip, TextClip, CompositeVideoClip
    )

    audio = AudioFileClip(audio_path)
    duration = audio.duration

    img_duration = duration / len(images) if images else duration

    clips = []
    for img_path in images:
        try:
            img_clip = ImageClip(img_path, duration=img_duration)
            img_clip = img_clip.resized(height=720)
            clips.append(img_clip)
        except Exception as e:
            print(f"  âš ï¸ ç”»åƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    if not clips:
        clips = [ColorClip(size=(1280, 720), color=(0,0,0), duration=duration)]

    video = concatenate_videoclips(clips, method="compose")

    # Add subtitles if script is provided
    if script:
        subtitles = split_script_into_subtitles(script)
        if subtitles:
            subtitle_duration = duration / len(subtitles)
            subtitle_clips = []

            for i, text in enumerate(subtitles):
                start_time = i * subtitle_duration

                try:
                    # Create text clip with Japanese font
                    txt_clip = TextClip(
                        text=text,
                        font='/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',
                        font_size=36,
                        color='white',
                        bg_color='black',
                        size=(1200, None),
                        method='caption',
                        text_align='center'
                    )
                    txt_clip = txt_clip.with_duration(subtitle_duration)
                    txt_clip = txt_clip.with_start(start_time)
                    txt_clip = txt_clip.with_position(('center', 620))
                    subtitle_clips.append(txt_clip)
                except Exception as e:
                    print(f"  âš ï¸ å­—å¹•ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")

            if subtitle_clips:
                video = CompositeVideoClip([video] + subtitle_clips)

    video = video.with_audio(audio)

    video.write_videofile(
        output_path,
        fps=24,
        codec='libx264',
        audio_codec='aac'
    )

    audio.close()
    return True

def get_youtube_credentials(channel_id):
    """Get YouTube OAuth credentials for the channel.

    Supports two token formats:
    1. JSON format: {"refresh_token": "...", "client_id": "...", "client_secret": "..."}
    2. Simple string: 1//0e... (refresh_token only, uses YOUTUBE_CLIENT_ID/SECRET env vars)

    Each channel (1-27) uses its own TOKEN_{channel_id} environment variable.
    """
    token_env_name = f'TOKEN_{channel_id}'

    token_value = os.environ.get(token_env_name)
    if not token_value:
        raise ValueError(f"{token_env_name} not found in environment variables")

    # Default credentials from environment
    default_client_id = os.environ.get('YOUTUBE_CLIENT_ID', '')
    default_client_secret = os.environ.get('YOUTUBE_CLIENT_SECRET', '')

    # Check if token is JSON format or simple string
    token_value = token_value.strip()
    if token_value.startswith('{'):
        # JSON format
        token_data = json.loads(token_value)
        refresh_token = token_data.get('refresh_token')
        client_id = token_data.get('client_id') or default_client_id
        client_secret = token_data.get('client_secret') or default_client_secret
    else:
        # Simple string format (refresh_token only)
        refresh_token = token_value
        client_id = default_client_id
        client_secret = default_client_secret

    if not client_id or not client_secret:
        raise ValueError("YOUTUBE_CLIENT_ID and YOUTUBE_CLIENT_SECRET must be set")

    creds = OAuthCredentials(
        token=None,
        refresh_token=refresh_token,
        token_uri='https://oauth2.googleapis.com/token',
        client_id=client_id,
        client_secret=client_secret,
        scopes=['https://www.googleapis.com/auth/youtube.upload']
    )

    return creds

def upload_to_youtube(file_path, title, description, channel_id, privacy='private'):
    """Upload video to YouTube and return the video URL."""
    creds = get_youtube_credentials(channel_id)
    youtube = build('youtube', 'v3', credentials=creds)

    # Prepare video metadata
    body = {
        'snippet': {
            'title': title,
            'description': description,
            'tags': ['æ˜­å’Œ', 'æ‡ã‹ã—ã„', 'ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'æ€ã„å‡º'],
            'categoryId': '22'  # People & Blogs
        },
        'status': {
            'privacyStatus': privacy,  # 'private', 'unlisted', or 'public'
            'selfDeclaredMadeForKids': False
        }
    }

    media = MediaFileUpload(
        file_path,
        mimetype='video/mp4',
        resumable=True,
        chunksize=1024*1024  # 1MB chunks
    )

    request = youtube.videos().insert(
        part='snippet,status',
        body=body,
        media_body=media
    )

    response = None
    while response is None:
        status, response = request.next_chunk()
        if status:
            print(f"    ğŸ“¤ {int(status.progress() * 100)}%")

    video_id = response.get('id')
    video_url = f"https://www.youtube.com/watch?v={video_id}"

    return video_url

def update_sheet_status(sh, row_num, status, video_url=''):
    """Update status and YouTube URL in spreadsheet."""
    ws = sh.worksheet('ãƒã‚¿ç®¡ç†')
    ws.update_cell(row_num, 6, status)
    if video_url:
        ws.update_cell(row_num, 9, video_url)

def process_channel(sh, channel_id, channel_info):
    """Process a single channel: generate and upload video.

    Args:
        sh: Spreadsheet object
        channel_id: Channel ID (1-27)
        channel_info: Dict with channel info from spreadsheet

    Returns:
        tuple: (success: bool, video_url: str or None, error_msg: str or None)
    """
    channel_name = channel_info.get('name') or CHANNELS.get(channel_id, f"ãƒãƒ£ãƒ³ãƒãƒ«{channel_id}")

    # Check if token exists for this channel
    token_env_name = f'TOKEN_{channel_id}'
    if not os.environ.get(token_env_name):
        return (False, None, f"TOKEN_{channel_id}ãŒæœªè¨­å®š")

    # Get pending neta for this channel
    neta = get_pending_neta(sh, channel_id)
    if not neta:
        return (False, None, "æœªä½œæˆã®ãƒã‚¿ãªã—")

    print(f"  ğŸ“ ãƒã‚¿: {neta['title']}")
    update_sheet_status(sh, neta['row_num'], 'ä½œæˆä¸­')

    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            # 1. Generate script
            print("    ğŸ“ åŸç¨¿ç”Ÿæˆä¸­...")
            script = generate_ranking_content(neta)
            if not script:
                update_sheet_status(sh, neta['row_num'], 'ã‚¨ãƒ©ãƒ¼')
                return (False, None, "åŸç¨¿ç”Ÿæˆå¤±æ•—")
            print(f"    âœ… åŸç¨¿ç”Ÿæˆå®Œäº†ï¼ˆ{len(script)}æ–‡å­—ï¼‰")

            # 2. Generate audio
            print("    ğŸ¤ éŸ³å£°ç”Ÿæˆä¸­...")
            audio_path = os.path.join(tmpdir, "audio.mp3")
            if not generate_audio_google_tts(script, audio_path):
                update_sheet_status(sh, neta['row_num'], 'ã‚¨ãƒ©ãƒ¼')
                return (False, None, "éŸ³å£°ç”Ÿæˆå¤±æ•—")
            print("    âœ… éŸ³å£°ç”Ÿæˆå®Œäº†")

            # 3. Get images
            print("    ğŸ–¼ï¸ ç”»åƒå–å¾—ä¸­...")
            images = get_images_with_fallback(
                title=neta['title'],
                category=neta['category'],
                count=neta['ranking_num'],
                tmpdir=tmpdir
            )
            print(f"    âœ… ç”»åƒå–å¾—å®Œäº†ï¼ˆ{len(images)}æšï¼‰")

            # 4. Create video with subtitles
            print("    ğŸ¥ å‹•ç”»ç”Ÿæˆä¸­ï¼ˆå­—å¹•ä»˜ãï¼‰...")
            video_path = os.path.join(tmpdir, "output.mp4")
            if not create_video_with_moviepy(audio_path, images, neta['title'], video_path, script=script):
                update_sheet_status(sh, neta['row_num'], 'ã‚¨ãƒ©ãƒ¼')
                return (False, None, "å‹•ç”»ç”Ÿæˆå¤±æ•—")
            print("    âœ… å‹•ç”»ç”Ÿæˆå®Œäº†")

            # 5. Upload to YouTube
            print("    ğŸ“º YouTubeã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
            description = f"{script[:100]}...\n\n"
            description += f"ã€{channel_name}ã€‘\n"
            description += "#æ˜­å’Œ #æ‡ã‹ã—ã„ #ãƒ©ãƒ³ã‚­ãƒ³ã‚° #æ€ã„å‡º #ãƒ¬ãƒˆãƒ­"

            video_url = upload_to_youtube(
                file_path=video_path,
                title=neta['title'],
                description=description,
                channel_id=channel_id,
                privacy='private'
            )
            print(f"    âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {video_url}")

            update_sheet_status(sh, neta['row_num'], 'å®Œæˆ', video_url)
            return (True, video_url, None)

    except Exception as e:
        error_msg = str(e)
        print(f"    âŒ ã‚¨ãƒ©ãƒ¼: {error_msg}")
        try:
            update_sheet_status(sh, neta['row_num'], 'ã‚¨ãƒ©ãƒ¼')
        except:
            pass
        return (False, None, error_msg)


def main():
    print("=" * 60)
    print("ğŸ¬ 27ãƒãƒ£ãƒ³ãƒãƒ«å‹•ç”»ä¸€æ‹¬ç”Ÿæˆ")
    print("=" * 60)

    creds = get_credentials()
    gc = gspread.authorize(creds)
    sh = gc.open_by_key(SPREADSHEET_ID)

    # Get channel info from spreadsheet
    channel_info_map = get_channel_info_from_sheet(sh)

    # Track results
    results = {
        'success': [],
        'failed': [],
        'skipped': []
    }

    # Process each channel
    for channel_id in range(1, TOTAL_CHANNELS + 1):
        channel_info = channel_info_map.get(channel_id, {})
        channel_name = channel_info.get('name') or CHANNELS.get(channel_id, f"ãƒãƒ£ãƒ³ãƒãƒ«{channel_id}")

        print(f"\nğŸ“º [{channel_id}/27] {channel_name}")
        print("-" * 40)

        success, video_url, error_msg = process_channel(sh, channel_id, channel_info)

        if success:
            results['success'].append({
                'channel_id': channel_id,
                'name': channel_name,
                'url': video_url
            })
        elif error_msg == "æœªä½œæˆã®ãƒã‚¿ãªã—" or "TOKEN" in (error_msg or ""):
            results['skipped'].append({
                'channel_id': channel_id,
                'name': channel_name,
                'reason': error_msg
            })
        else:
            results['failed'].append({
                'channel_id': channel_id,
                'name': channel_name,
                'error': error_msg
            })

    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“Š å‡¦ç†çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    print(f"\nâœ… æˆåŠŸ: {len(results['success'])}ä»¶")
    for r in results['success']:
        print(f"   - [{r['channel_id']}] {r['name']}: {r['url']}")

    print(f"\nâ­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {len(results['skipped'])}ä»¶")
    for r in results['skipped']:
        print(f"   - [{r['channel_id']}] {r['name']}: {r['reason']}")

    print(f"\nâŒ å¤±æ•—: {len(results['failed'])}ä»¶")
    for r in results['failed']:
        print(f"   - [{r['channel_id']}] {r['name']}: {r['error']}")

    print("\n" + "=" * 60)
    total = len(results['success']) + len(results['failed']) + len(results['skipped'])
    print(f"ğŸ‰ å‡¦ç†å®Œäº†ï¼ æˆåŠŸ:{len(results['success'])} / ã‚¹ã‚­ãƒƒãƒ—:{len(results['skipped'])} / å¤±æ•—:{len(results['failed'])} / åˆè¨ˆ:{total}")
    print("=" * 60)

if __name__ == "__main__":
    main()
